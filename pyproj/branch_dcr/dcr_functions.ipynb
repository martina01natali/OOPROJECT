{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6ac25c-726b-41ef-8e67-c2f7dbaec963",
   "metadata": {},
   "source": [
    "# Class construction\n",
    "\n",
    "## Fundamental questions\n",
    "Here are the things I should ask myself and answer before starting to build something big and complex as a class.\n",
    "- [ ] do I have many things linked to each other?\n",
    "- [ ] what is the type of the objects that I want to aggregate?\n",
    "- [ ] what are the ways in which one could aggregate those objects?\n",
    "- [ ] \n",
    "\n",
    "## Implementation ideas [WIP] [SPARSE]\n",
    "- a class [SiPM] with an instance that can contain waveform data and the related timetable\n",
    "    - all the methods I created for waveform analysis could be linked to the waveform and timetable objects only\n",
    "- the class [SiPM] could also contain different instances for the sipm characterization (see iv_sipm_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ced9bb0-04d5-4ec4-99f8-10de3a0a8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import argrelextrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf13e22-b810-46e0-b488-e6498f8321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wf(fname):\n",
    "    \"\"\"Basic read function with automatic timestamp or wf file type detection.\n",
    "    \n",
    "    If the file provided is a timestamp file, follow this routine:\n",
    "    - open the file\n",
    "    - make a dataframe with the timestamps; the header is as default in line 0\n",
    "    - timestamps are relative and provide the time interval with respect to the previous trigger:\n",
    "       make them absolute by computing the cumulative of each timestamp with cumsum method of pd\n",
    "    \n",
    "    If the file provided is a waveform file, follow this routine:\n",
    "    - open the file\n",
    "    - detect the number of datapoints in each waveform, wf_datapoints = 6250\n",
    "    - detect end of header (\"TIME\") and make a dataframe with the wf data\n",
    "    \n",
    "    If file is not found, FileNotFoundError is raised automatically.\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    - fname: relative path of the file with respect to the current working directory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Pandas' dataframe of the data provided.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if re.search(\"time.csv\", fname): \n",
    "        timestamp_file_name = fname\n",
    "        timestamp_path = os.path.join(os.getcwd(),timestamp_file_name)\n",
    "        timestamp_table = pd.read_csv(timestamp_path) \n",
    "            # if file is not found, FileNotFoundError is raised automatically\n",
    "        timestamp_table.rename(columns = {'X: (s)': 'Event', 'Y: (Hits)':'Timestamp'}, inplace = True)\n",
    "        N_of_events = len(timestamp_table) # = 1000\n",
    "        timestamp_table[\"Timestamp\"] = timestamp_table[\"Timestamp\"].cumsum(axis=0)\n",
    "        \n",
    "        return timestamp_table\n",
    "        \n",
    "    elif re.search(\"wf.csv\", fname):\n",
    "        wf_file_name = fname\n",
    "        wf_path = os.path.join(os.getcwd(),wf_file_name)\n",
    "        wf_table = pd.DataFrame()\n",
    "        \n",
    "        with open(wf_file_name, 'r') as file_wf:\n",
    "            lines = file_wf.readlines()\n",
    "            for line_counter, line in enumerate(lines):\n",
    "                if line.startswith(\"Record Length\"): wf_datapoints = int(line.split(',')[-1])\n",
    "                if line.startswith(\"Horizontal\"): time_unit = str(line.split(',')[-1]).rstrip(\"\\n\")\n",
    "                if line.startswith(\"Vertical\"): ampl_unit = str(line.split(',')[-1]).rstrip(\"\\n\")\n",
    "                if line.startswith(\"FastFrame\"): wf_events = int(line.split(',')[-1])\n",
    "                if line.startswith(\"TIME\"):\n",
    "                    wf_table = pd.read_csv(wf_path, header = line_counter-1)\n",
    "                    break\n",
    "        \n",
    "        meta = {\n",
    "            'path' : wf_path,\n",
    "            'n events' : wf_events,\n",
    "            'data points' : wf_datapoints,\n",
    "            'time units' : time_unit,\n",
    "            'ampl units' : ampl_unit,}\n",
    "                    \n",
    "        return wf_table, meta\n",
    "    \n",
    "    else:\n",
    "        raise NameError(\"Please provide the path to a waveform or timestamp comma-separated file (.csv).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1fca159-4be4-4e26-98c6-938c096b9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(timestamp_table, wf_table, meta, custom_n_events=1000, time_adjust=True,\n",
    "             threshold=0.006, distance=50, many_minima=6250,\n",
    "             plot=False, save_plot=False):\n",
    "    \"\"\"Function to analyze waveform data and locate clean signal peaks.\n",
    "    \n",
    "    This function finds all the minima in each waveform (wf) and selects the \"good ones\" (clean_min) based on\n",
    "    threshold (V) and distance (#). The resulting dataframe has a column \"code\" that indicates if\n",
    "    a clean_min belongs to a good or bad wf: bad wfs are the ones containing a number of relative minima\n",
    "    bigger than many_minima or that contain -inf saturated data. Notice that the \"good\" or \"bad\" coding\n",
    "    makes sense for discriminating between equally clean_min only: discrimination is not provided for\n",
    "    minima that are not considered to be \"good\" signal.\n",
    "   \n",
    "   ------\n",
    "   Input:\n",
    "   - timestamp_table: pandas.DataFrame\n",
    "       Dataframe with timestamps\n",
    "   - wf_table: pandas.DataFrame\n",
    "       Dataframe with waveforms in list mode\n",
    "   - meta: dict\n",
    "       Dictionary with metadata of wf_table\n",
    "   - custom_n_events: int, default 1000\n",
    "       Number of events to analyze (starts from the first waveform in any case)\n",
    "   - time_adjust: bool, default True\n",
    "       If True, adds timestamps from timestamp_table to wf_table. May be kept\n",
    "       True for the first time the analysis is run on the dataset, switch\n",
    "       to False afterwards.\n",
    "   - threshold: float, default 0.006 [V]\n",
    "       Minimum value of signal to discriminate it from noise, in units of V\n",
    "   - distance: int, default 50\n",
    "       Number of data in between two consecutive absolute minima\n",
    "   - many_minima: int, default 6250 (# data in single waveform)\n",
    "       Provide a number of minimum minima to be found to turn on or off a warning \n",
    "       that is raised if in the waveform there are more than that number of minima.\n",
    "       Also provides additional column in dataframe with code \"bad_wf\" for\n",
    "       waveforms that satisfy the above condition.\n",
    "   - plot: bool, default False\n",
    "       If True, plot the scatterplot of the waveforms with the relative minima in a\n",
    "       recursive way\n",
    "   \n",
    "   --------\n",
    "   Returns:\n",
    "   - copy of original dataframe with added columns of minima (total) and clean minima (based\n",
    "       on threshold and distance)\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    N_of_events = custom_n_events        \n",
    "    wf_datapoints = len(wf_table)/1000\n",
    "    \n",
    "    copy = wf_table.copy()\n",
    "    general_clean_ampl = []\n",
    "    general_clean_min = []\n",
    "    general_bad = []\n",
    "    time_list = []\n",
    "    N_bad_wf = 0\n",
    "        \n",
    "    for n in range(N_of_events):\n",
    "        print(\"Analysis of event number \" + str(n+1), end='\\r') #'\\r' overwrites output\n",
    "        event_name = 'Event_' + str(n) + '.png'\n",
    "        \n",
    "        if time_adjust == True:\n",
    "            copy[\"TIME\"].loc[wf_datapoints*n: (wf_datapoints*(n+1))-1] += timestamp_table.at[n,\"Timestamp\"]            \n",
    "        \n",
    "        single_wf = copy.loc[wf_datapoints*n: (wf_datapoints*(n+1))-1].copy()\n",
    "        minimum_list = argrelextrema(single_wf.CH1.values, np.less_equal, order = distance)[0]\n",
    "        baseline = np.polyfit(single_wf[\"TIME\"].iloc[0:250], single_wf[\"CH1\"].iloc[0:250],0)[0]\n",
    "        time_list.append(single_wf.TIME.max()-single_wf.TIME.min())\n",
    "\n",
    "        gap = threshold\n",
    "        clean_minimum_list = []\n",
    "        previous_index     = minimum_list[0]\n",
    "        \n",
    "        for index in minimum_list:\n",
    "            if (baseline - single_wf[\"CH1\"].iat[index] > gap) and (index > previous_index + distance):\n",
    "                clean_minimum_list.append(index)\n",
    "                general_clean_ampl.append(baseline - single_wf[\"CH1\"].iat[index])\n",
    "                previous_index = index\n",
    "                        \n",
    "        inf_counts = 0\n",
    "        inf_counts = len(single_wf[single_wf.CH1==-np.inf])\n",
    "        if inf_counts > 0:\n",
    "            N_bad_wf += 1\n",
    "        \n",
    "        wf_index = (n*wf_datapoints)\n",
    "        for index in clean_minimum_list:\n",
    "            general_clean_min.append(index + wf_index)\n",
    "            \n",
    "            if len(minimum_list) > many_minima or inf_counts > 0: # implement or for wf containing -inf data \n",
    "                general_bad.append(index + wf_index)\n",
    "        \n",
    "        # Plotting control (inside loop)\n",
    "        if plot==True:\n",
    "            single_wf.loc[:,'min'] = single_wf.iloc[minimum_list]['CH1']\n",
    "            single_wf.loc[:,'clean_min'] = single_wf.iloc[clean_minimum_list]['CH1']\n",
    "            plt.plot(single_wf[\"TIME\"], single_wf['CH1'], linestyle=\"-\", linewidth=1)\n",
    "            plt.scatter(single_wf[\"TIME\"], single_wf['min'], color=\"darkred\")\n",
    "            plt.scatter(single_wf[\"TIME\"], single_wf['clean_min'], color=\"green\")\n",
    "            plt.axhline(baseline, c='b')\n",
    "            plt.show()\n",
    "            \n",
    "        if save_plot==True:\n",
    "            figure_path = os.path.join(os.path.join(os.getcwd(),'grafici'), event_name)\n",
    "            plt.savefig(figure_path)\n",
    "            plt.close()\n",
    "    \n",
    "    # Metadata update\n",
    "    total_time = sum(time_list)\n",
    "    meta['total acquis time'] = total_time\n",
    "    meta['n clean minima'] = len(general_clean_min)-len(general_bad)\n",
    "    meta['n bad minima'] = len(general_bad)\n",
    "    meta['DCR'] = meta[\"n clean minima\"]/total_time\n",
    "    \n",
    "    # Printed output\n",
    "    print('\\nAnalysis completed.')\n",
    "    print('Number of clean minima found: ', len(general_clean_min)-len(general_bad))\n",
    "    print('Fraction of waveforms with too many minima or -inf data (\"bad_wf\") on total: %s%%' % format(N_bad_wf/custom_n_events*100,\".2f\"))\n",
    "    print('Total acquisition time: {0:0.3e} s'.format(total_time))\n",
    "    print('Estimated DCR: {0:0.3e} Hz'.format(meta[\"n clean minima\"]/total_time))\n",
    "        \n",
    "    # Return control\n",
    "    clean_ampl = pd.DataFrame(general_clean_ampl, index=general_clean_min, columns=['ampl_min'])\n",
    "    if many_minima < 6250 or len(general_bad) > 0:\n",
    "        bad_list = ['bad_wf' for item in general_bad]\n",
    "        bad = pd.DataFrame(bad_list, index=general_bad, columns=['code'])\n",
    "        \n",
    "    copy.loc[:,'clean_min'] = copy.iloc[general_clean_min]['CH1']\n",
    "    copy = copy.join(clean_ampl)\n",
    "    copy = copy.join(bad)\n",
    "    copy.code.fillna(value='good', inplace=True)\n",
    "    copy['wfID'] = np.array(range(len(copy))) // 6250\n",
    "    copy.set_index('wfID', append=True, inplace=True)\n",
    "    print(\"Process completed in %s s.\" % (format(time.time()-start_time,\".2f\")))\n",
    "    return copy, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15de0219-4f21-4791-bfef-90e22ad00bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analysis_delta_t(analyzed_wf, meta,\n",
    "                    noise_list=['primary dark counts', 'delayed crosstalk', 'crosstalk', 'afterpulses'],\n",
    "                    crosstalk_thr=10e-3, delayed_cross_thr=6e-6,\n",
    "                    ):\n",
    "    \"\"\"Function to polish the dataframe returned from the analysis function and discriminate between noise.\n",
    "    \n",
    "    \"\"\"\n",
    "    def noise_discrimination(df, crosstalk_thr, delayed_cross_thr):\n",
    "            primary_mean = df.groupby(by=[df['Amplitude (V)'] < crosstalk_thr]).get_group(True)['Amplitude (V)'].mean()\n",
    "            primary_std = df.groupby(by=[df['Amplitude (V)'] < crosstalk_thr]).get_group(True)['Amplitude (V)'].std()\n",
    "            df['Noise'] = (\n",
    "                np.where(\n",
    "                    df['Amplitude (V)'] > crosstalk_thr , 'crosstalk',\n",
    "                    np.where(df['Delta T (s)'] < delayed_cross_thr, 'delayed crosstalk', 'primary dark counts')))\n",
    "            df.loc[df['Amplitude (V)'] < (primary_mean-3*primary_std)]['Noise'].apply(lambda x : 'afterpulses')\n",
    "            return df\n",
    "\n",
    "    mins = analyzed_wf.dropna() # drops all rows with NaNs, that are found in clean_min only\n",
    "    mins = mins.loc[mins.code=='good']\n",
    "    mins['TIME'] = mins['TIME'].diff(periods=1)\n",
    "    mins = mins.iloc[1:,[0,-2]]\n",
    "    mins.rename(columns={'TIME':'Delta T (s)','ampl_min':'Amplitude (V)'}, inplace = True)\n",
    "    \n",
    "    mins = noise_discrimination(mins, crosstalk_thr=crosstalk_thr,\n",
    "                                delayed_cross_thr=delayed_cross_thr)\n",
    "    \n",
    "    return mins, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba91bb0-44b0-4b8e-aba0-2d645b9c1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(data, sns_palette='deep', title='2D plot',\n",
    "            show=True, save=False, save_path='./Amplitude_vs_dt.', save_extension='pdf',\n",
    "            **kwargs,):\n",
    "    \"\"\"2D plot with amplitude (V) vs time delta (s) scatterplot and kernel density estimation.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    from matplotlib.lines import Line2D\n",
    "            \n",
    "    f, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True, sharey=False)\n",
    "    ax1 = axs[0]\n",
    "    ax2 = axs[1]\n",
    "    \n",
    "    mins = data\n",
    "    sns.scatterplot(data=mins, x=\"Delta T (s)\", y=\"Amplitude (V)\", hue=\"Noise\", ax=ax1,\n",
    "                    alpha=0.7, legend=False, palette=sns_palette)\n",
    "    sns.kdeplot(data=mins, x=\"Delta T (s)\", hue=\"Noise\", ax=ax2,\n",
    "                 multiple=\"stack\", fill=True, log_scale=True, common_norm=True,\n",
    "                 edgecolor='white', alpha=0.7, palette=sns_palette,\n",
    "                 legend=False)\n",
    "    \n",
    "    ax1.set_title(title, fontsize=14)\n",
    "    ax1.set_xscale('log')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_ylabel('Density (%)')\n",
    "    \n",
    "    n_mins = len(mins)\n",
    "    noise_list_red = mins.groupby('Noise').count().index.values\n",
    "    noise_list_red = sorted(noise_list_red, reverse=True)\n",
    "    \n",
    "    noise_mean_t = [mins.groupby('Noise').mean().loc[i]['Delta T (s)'] for i in noise_list_red]\n",
    "    noise_percent = [mins.groupby('Noise').count().loc[i].values[0]/n_mins*100 for i in noise_list_red]\n",
    "    \n",
    "    deep_cmap = sns.color_palette(sns_palette, 10)\n",
    "    palette = sns.color_palette([deep_cmap[i] for i in range(len(noise_list_red))])\n",
    "    \n",
    "    legend_scatter = [Line2D([0], [0], marker='o', color='w', label=f'{noise_list_red[i]}',\n",
    "                              markerfacecolor=palette[i], markersize=8) for i in range(len(noise_list_red))]\n",
    "    \n",
    "    legend_kde = [Line2D([0], [0], marker='o', color='w', label=f'{noise_list_red[i]} '+'({0:0.1f}%)'.format(noise_percent[i]),\n",
    "                              markerfacecolor=palette[i], markersize=8) for i in range(len(noise_list_red))]\n",
    "    \n",
    "    ax1.legend(handles=legend_scatter, loc='upper left')\n",
    "    ax2.legend(handles=legend_kde, loc='upper left') #bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    if show==True:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    if save==True:\n",
    "        f.savefig(save_path+save_extension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oop",
   "language": "python",
   "name": "oop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
